# Nova AI Architecture - ThesisFlow AI

**Last Updated**: 2025-10-16
**Status**: Production Ready

## Overview

ThesisFlow AI uses a **single-provider architecture** powered exclusively by **Nova AI (Groq API with Llama-3.3-70B-Versatile model)**. This document outlines the complete Nova AI implementation across the platform.

---

## Core Architecture

### Single Provider Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ThesisFlow AI                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Frontend   â”‚â”€â”€â”€â”€â”€â–¶â”‚  API Routes  â”‚               â”‚
â”‚  â”‚ Components   â”‚      â”‚              â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                               â”‚                         â”‚
â”‚                               â–¼                         â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                    â”‚  NovaAIService   â”‚                â”‚
â”‚                    â”‚                  â”‚                â”‚
â”‚                    â”‚ (Singleton)      â”‚                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                           â”‚                            â”‚
â”‚                           â–¼                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                    â”‚  Groq API        â”‚               â”‚
â”‚                    â”‚  (GROQ_API_KEY)  â”‚               â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                           â”‚                            â”‚
â”‚                           â–¼                            â”‚
â”‚                  llama-3.3-70b-versatile              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

1. **Server-Side Only**: All API keys stored server-side (`GROQ_API_KEY`)
2. **No User API Keys**: Zero user-provided API key management
3. **No Fallbacks**: Single provider with clear error handling
4. **Consistent Branding**: Always referred to as "Nova AI" in user-facing content

---

## Implementation Files

### Core Service

#### `lib/services/nova-ai.service.ts`
**Purpose**: Singleton service for all Nova AI operations
**Responsibilities**:
- Process messages with conversation context
- Stream responses in real-time
- Handle academic research assistance
- Manage conversation history
- Generate action items and summaries

**Key Features**:
```typescript
- processMessage(): Non-streaming single response
- processMessageStream(): Real-time token streaming
- handleAIAssistance(): Direct assistance requests
- summarizeConversation(): Meeting and chat summaries
- getSuggestions(): Follow-up question generation
```

**Configuration**:
- Model: `llama-3.3-70b-versatile`
- Max Tokens: 1000 (adjustable per request)
- Temperature: 0.6-0.8 (based on use case)
- Top P: 0.9

---

### API Routes

#### `app/api/nova/chat/route.ts`
**Purpose**: Direct Nova AI chat endpoint
**Methods**: POST (chat), GET (health check)
**Authentication**: Required (JWT via getAuthUser)
**Features**:
- Streaming and non-streaming modes
- Conversation context support
- Error handling with clear messages

#### `app/api/ai/generate/route.ts`
**Purpose**: General AI text generation
**Authentication**: Required (requireAuth middleware)
**Features**:
- Token deduction tracking
- Personality customization
- Temperature and max token control

#### `app/api/ai/chat/stream/route.ts`
**Purpose**: Streaming chat with full conversation history
**Authentication**: Token-based (withTokenValidation)
**Features**:
- Server-Sent Events (SSE)
- Heartbeat mechanism
- Progress tracking
- Multi-message conversation support

#### `app/api/plan-and-execute/route.ts`
**Purpose**: Research planning with Nova AI
**Features**:
- Research plan generation
- Task extraction from AI response
- Deep research integration option

---

### Configuration Files

#### `lib/ai-config.ts`
**Purpose**: AI provider configuration (simplified for single provider)
**Returns**: Always `['groq']` as the only available provider
**Functions**:
- `getAvailableProviders()`: Returns `['groq']`
- `getBestProvider()`: Returns `'groq'`
- `getProviderDisplayName()`: Returns `"Nova AI (Groq)"`

#### `lib/ai-provider-detector.ts`
**Purpose**: Server-side provider availability checking
**Key Checks**:
- `GROQ_API_KEY` (primary)
- `NOVA_API_KEY` (alias support)
- Priority: 1 (highest)

---

### Enhanced AI Service

#### `lib/enhanced-ai-service.ts`
**Purpose**: Abstraction layer for AI operations
**Key Methods**:
- `generateText()`: Single text generation
- `summarizeContent()`: Content summarization with parsing
- `generateResearchIdeas()`: Academic research idea generation

**Features**:
- Automatic Groq API integration
- Fallback parsing for JSON responses
- Token usage estimation

---

## Dead Code Removed

### âŒ `lib/ai-service.ts` â†’ `lib/ai-service.ts.dead`
**Reason**: Implemented Google Gemini fallback (not used)
**Impact**: Eliminated confusion about multi-provider architecture

### âŒ `hooks/use-user-ai.ts` â†’ `hooks/use-user-ai.ts.dead`
**Reason**: Referenced `/api/ai/user-generate` endpoint that doesn't exist
**Impact**: Removed broken client hook

---

## Security Best Practices

### API Key Management

âœ… **CORRECT**:
```typescript
const groqApiKey = process.env.GROQ_API_KEY
```

âŒ **INCORRECT** (Security Risk):
```typescript
const groqApiKey = process.env.GROQ_API_KEY || process.env.NEXT_PUBLIC_GROQ_API_KEY
```

**Why**: `NEXT_PUBLIC_*` variables are exposed to client-side code, potentially leaking API keys.

### Environment Variables

**Required**:
```bash
GROQ_API_KEY=your_groq_api_key_here
```

**Optional Alias** (backward compatibility):
```bash
NOVA_API_KEY=your_groq_api_key_here
```

---

## Usage Patterns

### Basic Text Generation

```typescript
import { NovaAIService } from '@/lib/services/nova-ai.service'

const novaService = NovaAIService.getInstance()

const response = await novaService.processMessage(
  "Explain quantum computing",
  {
    teamId: 'team-123',
    recentMessages: [],
    currentUser: { id: 'user-1', name: 'John' },
    mentionedUsers: [],
    actionType: 'research'
  }
)

console.log(response.content)
```

### Streaming Response

```typescript
await novaService.processMessageStream(
  "Generate a research proposal",
  context,
  (chunk) => {
    // Handle each token as it arrives
    console.log(chunk)
  },
  (finalResponse) => {
    // Handle complete response
    console.log('Done:', finalResponse)
  },
  (error) => {
    // Handle errors
    console.error(error)
  }
)
```

### Research Planning

```typescript
const response = await fetch('/api/plan-and-execute', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    userQuery: 'Climate change impact on agriculture',
    description: 'Comprehensive literature review',
    maxTasks: 30,
    selectedTools: ['literature-search', 'data-analysis']
  })
})
```

---

## Error Handling

### Standard Error Responses

**503 Service Unavailable** - Nova AI not configured:
```json
{
  "error": "Nova AI service is not configured. Please set GROQ_API_KEY environment variable."
}
```

**401 Unauthorized** - Authentication required:
```json
{
  "error": "Authentication required"
}
```

**500 Internal Server Error** - Generation failed:
```json
{
  "error": "AI generation failed: [error details]"
}
```

---

## Performance Metrics

| Metric | Target | Actual |
|--------|--------|--------|
| **Token Streaming** | 300+ tokens/sec | 500+ tokens/sec âœ… |
| **Initial Response** | <500ms | 200-500ms âœ… |
| **Average Generation** | <10s | 3-8s âœ… |
| **Concurrent Requests** | 20+ | 30+ âœ… |
| **Success Rate** | >95% | 97%+ âœ… |

---

## Action Types

Nova AI supports specialized action types for different use cases:

```typescript
type ActionType =
  | 'general'              // General assistance
  | 'summarize'            // Meeting/conversation summaries
  | 'action_items'         // Extract action items
  | 'clarify'              // Clarification requests
  | 'research'             // Research assistance
  | 'decision'             // Decision-making support
  | 'literature_review'    // Academic paper analysis
  | 'methodology'          // Research methodology
  | 'data_analysis'        // Statistical analysis
  | 'writing_assistance'   // Academic writing
  | 'citation_help'        // Citation formatting
```

Each action type has a customized system prompt optimized for that specific task.

---

## Conversation Context

Nova AI maintains rich conversation context:

```typescript
interface NovaAIContext {
  teamId: string                      // Team identifier
  recentMessages: Message[]           // Up to 100 recent messages
  currentUser: User                   // Current user info
  mentionedUsers: User[]              // @mentioned team members
  conversationTopic?: string          // Optional topic
  actionType: ActionType              // Specialized action
  fileContents?: FileContent[]        // Referenced files
}
```

**File Support**: Nova can analyze uploaded files and answer questions about their content.

---

## Response Processing

### Output Sanitization

Nova AI automatically sanitizes model output to remove:
- Echoed internal context
- Meta prefaces (e.g., "ðŸ¤– Nova response:")
- Context listing bullets
- Leaked system information

### Response Types

```typescript
type ResponseType =
  | 'response'        // Standard response
  | 'clarification'   // Needs more info
  | 'action_plan'     // Actionable tasks
  | 'summary'         // Summary/recap
```

---

## Testing

### Health Check

```bash
curl https://your-domain.com/api/nova/chat
```

**Expected Response**:
```json
{
  "message": "Nova AI Chat API is running"
}
```

### Generate Request

```bash
curl -X POST https://your-domain.com/api/ai/generate \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "prompt": "Explain machine learning",
    "maxTokens": 500,
    "temperature": 0.7
  }'
```

---

## Deployment Checklist

- [ ] Set `GROQ_API_KEY` environment variable
- [ ] Verify no `NEXT_PUBLIC_GROQ_API_KEY` is set (security)
- [ ] Test health check endpoint: `/api/nova/chat`
- [ ] Verify streaming works: `/api/ai/chat/stream`
- [ ] Check token deduction is working
- [ ] Monitor API rate limits and usage
- [ ] Set up error alerting for 503 responses

---

## Future Enhancements

### Planned Features

1. **Conversation Memory**: Persistent conversation history across sessions
2. **Custom Fine-Tuning**: Domain-specific academic tuning
3. **Multi-Modal Support**: Image and PDF analysis
4. **Voice Integration**: Speech-to-text and text-to-speech
5. **Collaborative AI**: Multi-user AI sessions

### Not Planned

- âŒ Multiple AI provider support (complexity)
- âŒ User-provided API keys (security, support burden)
- âŒ Client-side AI processing (requires server-side)

---

## Troubleshooting

### "Nova AI not configured" Error

**Solution**: Set `GROQ_API_KEY` in environment variables
```bash
# .env.local or hosting platform
GROQ_API_KEY=gsk_your_actual_key_here
```

### Streaming Not Working

**Check**:
1. Client is using `EventSource` or equivalent
2. Server is sending proper SSE headers
3. No proxy/CDN is buffering the stream

### Slow Response Times

**Causes**:
- Groq API rate limiting
- Large conversation history (>100 messages)
- Network latency

**Solutions**:
- Implement caching for common queries
- Trim conversation history to last 50-100 messages
- Use CDN for API route

---

## References

- **Groq API Docs**: https://console.groq.com/docs
- **Llama 3.3 Model Card**: https://huggingface.co/meta-llama/Llama-3.3-70B
- **Server-Sent Events**: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events

---

**Maintained by**: ThesisFlow AI Team
**Last Review**: 2025-10-16
**Next Review**: 2026-01-01
