# Supabase Configuration - REQUIRED FOR CHAT FUNCTIONALITY
NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# Socket.io Configuration
SOCKET_PORT=3001
NEXT_PUBLIC_SOCKET_URL=ws://localhost:3001

# App Configuration
NEXT_PUBLIC_APP_URL=http://localhost:3000

# AI Detection Configuration - REQUIRED FOR REAL FUNCTIONALITY
# Get your API key from https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=your_huggingface_api_key_here

# Optional: Use ensemble of multiple models for higher accuracy (default: true)
# When true, queries 4 state-of-the-old models and combines results
AI_DETECT_USE_ENSEMBLE=true

# Optional: Single model to use when ensemble is disabled
# Only used when AI_DETECT_USE_ENSEMBLE=false
HUGGINGFACE_DETECT_MODEL=openai-community/roberta-large-openai-detector

# Optional: Detection threshold (0.0 to 1.0, default: 0.5)
# Lower values = more likely to flag as AI
AI_DETECT_THRESHOLD=0.5

# Optional: Enable debug logging for AI detection
AI_DETECT_DEBUG=false

# NOTE: Without HUGGINGFACE_API_KEY, the AI detection will fall back to heuristics only

# ENSEMBLE MODELS (automatically used when AI_DETECT_USE_ENSEMBLE=true):
# 1. openai-community/roberta-large-openai-detector (35% weight) - Best for GPT-2/3
# 2. Hello-SimpleAI/chatgpt-detector-roberta (25% weight) - Optimized for ChatGPT
# 3. umm-maybe/AI-text-detector (20% weight) - General AI detection
# 4. PirateXX/AI-Content-Detector (20% weight) - Multi-model detector

# SINGLE MODEL OPTIONS (for AI_DETECT_USE_ENSEMBLE=false):
# - openai-community/roberta-base-openai-detector (faster, less accurate)
# - openai-community/roberta-large-openai-detector (slower, more accurate)
# - Hello-SimpleAI/chatgpt-detector-roberta (best for ChatGPT)
# - umm-maybe/AI-text-detector (balanced performance)

# Literature Search Debugging
# Enable detailed logs for rate limiting + retry policies in LiteratureSearchService
# Set to 1 to enable
LIT_SEARCH_DEBUG=0

# Optional: Google Scholar search via SerpAPI
# Get a free key at https://serpapi.com
SERPAPI_KEY=your_serpapi_key_here

# Unified Search Integrations (optional but recommended)
# Google Custom Search for web results
GOOGLE_SEARCH_API_KEY=your_google_cse_api_key
GOOGLE_SEARCH_CSE_ID=your_google_cse_id

# Tavily Web Search
TAVILY_API_KEY=your_tavily_api_key

# LangSearch (web/scholar/news/code/docs)
LANGSEARCH_API_KEY=your_langsearch_api_key
# Optional custom API URL (defaults to https://api.langsearch.io/v1)
# LANGSEARCH_API_URL=https://api.langsearch.io/v1

# Context7 MCP (documentation/library search)
# Provide either CONTEXT7_API_URL or CONTEXT7_MCP_URL
CONTEXT7_API_URL=https://api.context7.dev
# CONTEXT7_MCP_URL=
CONTEXT7_API_KEY=your_context7_api_key

# News fallback (GNews)
GNEWS_API_KEY=your_gnews_api_key

# Nova AI Configuration (Research Assistant)
# Get your API key from https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# AI Assistant Model Configuration
# Configure which models to use for each assistant feature
# All models use Groq API (requires GROQ_API_KEY above)

# Research Assistant - Fast web search + lightweight tools
RESEARCH_ASSISTANT_MODEL=groq/compound-mini

# Deep Research - Browser search for interactive browsing
DEEP_RESEARCH_MODEL=gpt-oss-120b

# Topic Reports Curator - Fast source ranking
TOPIC_REPORT_CURATOR_MODEL=llama-3.1-8b-instant

# Topic Reports Synthesizer - Complex synthesis with browser search
TOPIC_REPORT_SYNTHESIZER_MODEL=gpt-oss-120b

# Mathematical Analysis - Wolfram Alpha integration
MATHEMATICAL_ANALYSIS_MODEL=groq/compound

# Data Extraction - Code execution + web verification
DATA_EXTRACTION_MODEL=groq/compound-mini

# NOVA Chat - Fast instruct-following for conversations
NOVA_CHAT_MODEL=llama-3.1-8b-instant

# Summarization - 128K context with reasoning effort
SUMMARIZATION_MODEL=gpt-oss-120b

# Paraphrasing - Ultra-fast text variations
PARAPHRASING_MODEL=llama-3.1-8b-instant

# Y.js WebSocket Configuration (Collaborative Editing)
# For local development, use ws://localhost:1234
# For production, use wss:// with your WebSocket server URL
NEXT_PUBLIC_YJS_WEBSOCKET_URL=ws://localhost:1234
